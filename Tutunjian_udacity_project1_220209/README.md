# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**
The dataset contains information about costumers to a bank. The information include data about thier age, education, if they have a loan, thier jobs and etc. The last column y includes information if the costumer should recieve marketing advertisement or not.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
The Scikit Learn LinearRegression model had an accuracy result 0.9170, whereas the autoML resulted an accuracy of 0.9178 slightly better than the LinearRegression. As a result, the best model is VotingEnsemble obtained by autoML.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
The Scikit-Learn approach uses the LogisticRegression algorithm to train and build a model which predicts the descision for marketing.
although it is called  "regression", however the algorithm serves as a binary classification. the hyper parameters chosen for the optimization were C and maximum iterations. the choices of values for the paramter tuning were selected randomly. The RandomParameterSampling was choosen here. The model was prepared in a python script and saved in the Notebook, the hyperdrive then used the script, the sampler and the dataset (which was converted through the python script into input x and label y) and choose randomly 20 combination of C and maximum iterations to find the best accuracy score of the mode. the best model was choosen and saved in the model directory as "sklearn-best-model.joblib".
![alt text](https://github.com/shahan00t/udacity_project1_resubmission/blob/main/project1_sklearn_architecture.jpg?raw=true)

**What are the benefits of the parameter sampler you chose?**
RandomParameterSampling in contrast to Grid sampling is a faster and efficient way to optimize the hyperparameters. Unlike the grid approach it doesent choose all the combinations, but rather randomly a set of arguments.

**What are the benefits of the early stopping policy you chose?**
The early stopping policy used here assumes if after 2 successive attempts the model does not improve 10% it wil break the attempt in order to save time and resources.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
The autoML used the pre specified 30 minute of time to test different calssification algorithems to find the best model with the highest accuracy score. It itterates through the known algorithms in Azure to find the optimum for the given dataset. it even automatically splits the data into test and train.

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
The two models both do binary classification, they both had similar accuracy scores. The VotingEnsemble uses in itself a conceptually combination of different models and uses the majoprity of voting the prediction. the logisticRegression however, is a linear model for classification.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
for future improvements, if more time and resources are available a grid sampler can be implemented with more coices for the parameters in the hypertimng approach. for the autoML a longer experiment_timeout can be set to test more algorithms an achieve better accuracy.

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
cluster is running on company resources deletion is not reuired. Nevertheless, to complete the requirements of the udacity project, the screenshot confirming the deletion is added!
![alt text](https://github.com/shahan00t/udacity_project1_resubmission/blob/main/Udacity_computercluster_delete.jpg?raw=true)
